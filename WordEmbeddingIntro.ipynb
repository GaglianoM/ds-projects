{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><span style=\"color: #333399;\">Computational Linguistics: Text Vectorization and Word Embedding</span></h1>\n",
    "\n",
    "<h6 style=\"text-align: center;\">Created by: Michael Gagliano on 2/8/19</h6>\n",
    "<h6 style=\"text-align: center;\">Last Update: Michael Gagliano 2/12/19</h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Background Information</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Linguistics and Natural Language Processing are interdisciplinary fields of study drawing from speech linguistics, phonology, cognitive science, neurolingustics, computer science, machine learning, deep learning, pragmatics - and many more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">Why is NLP important?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most significant problems in NLP (Natural Language Processing) - and a paradigm of programming in general - is the problem of ***ambiguity.***  \n",
    "\n",
    "Natural language is abstract. Humans are great at inferring meaning from speech and text. Computers? Not so much.\n",
    "\n",
    "**The goal of Computational Linguistics - in an extremely simplified sense - is making computers and software understand natural language. This means both speech and text.**\n",
    "\n",
    "If you are designing a car, and someone tells you to \"make it fast\", how do we determine that fast means? Acceleration? Top speed? Relative to a turtle, my car, or an F1 car? \n",
    "\n",
    "Programming works to solve these abstract ideas and concepts by turning them into something explicity defined - much like a how following a recipe works.\n",
    "\n",
    "Below are additional specific examples of linguistic elements that humans are inherently good at understanding, but computers are not.\n",
    "\n",
    ">**Polysemy** is the coexistence of multiple meanings for one word   \n",
    "    *e.g. \"My mouth is sore\" vs. \"Watch your mouth\"*\n",
    "\n",
    ">**Implication** is speech that indirectly assumes meaning through inference  \n",
    "    *e.g. \"The sky looks really bad\" can imply bad weather is coming*\n",
    "\n",
    ">**Figurative Language/Tropes** such as metaphors, metonyms, and idioms provide symbolic meaning through comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Word Embedding</h1>\n",
    "\n",
    "\n",
    "[Word2Vec](#Word2Vec)\n",
    "[Doc2Vec](#Doc2Vec)\n",
    "[GloVe](#GloVe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> \"**Word Embedding** is a representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together. In the deep learning frameworks such as TensorFlow, Keras, this part is usually handled by an **embedding layer** which stores a lookup table to map the words represented by numeric indexes to their dense vector representations.\"\n",
    "\n",
    "Source: https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplified:**\n",
    "\n",
    "> Word embedding is a language modeling technique that takes words found in text documents and maps them to a vector form of real numbers using distributed representation. Consider this \"translating\" the words to a form the computer can then use for natural language processing while capturing them with as much of the semantical/morphological/context/hierarchical/etc. information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">How does it work?</h1>\n",
    "\n",
    "Before we get deep into how word embedding works and the role it plays in deep learning, let's start by understanding how we get text documents in a form that can be intepreted and utilized for natural language processing as whole. \n",
    "\n",
    "Below is an introduction on how text corpora undergo vectorization so they can be interpreted and analyzed, starting with two primitive methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Text Vectorization</h2>\n",
    "\n",
    "<h4 style=\"text-align: center;\"><span style=\"color: red\">Note: This is not yet word embedding. This section is for understanding the most common methods of vectorization of text corpora (CountVectorizer/Binarized OneHotEncoding, and TF-IDF) but do not give a <u>distributed representation</u>, which is essential to word embedding techniques</span></h4>\n",
    "\n",
    "**What:** Text Vectorization is a specific type of <i>Feature Extraction</i> in NLP. By allowing us to represent documents numerically, we gain the power to perform analytics on the data as well as create instances that are fed into machine learning algorithms, like Word Embedding. Whether you are using a pre-designed vectorization method (i.e. CountVectorizer) or build a custom feature extraction algorithm, the functional output represents the attributes and properties of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measures:** Word Frequency via unigrams\n",
    "\n",
    "\n",
    "**Why:** Document classification. Compare document similarities based on distance between matrices.  \n",
    "\n",
    "<span style=\"color: red\"><b>Does not account for:</b> Syntax, semantics</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. SciKit-Learn Count Vectorizer ([Bag-of-Words Approach][Ref1])\n",
    "\n",
    "\n",
    "\n",
    "[Ref1]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "This is a very simple and naive method of word embedding. SciKit-Learn's CountVectorizer function creates a sparse matrix where the words and documents that have effectively become vectors are stored. \n",
    "\n",
    "<i>IMPORTANT: CountVectorizer tokenizes the corpus for you, whereas other methods such as NLTK and Gensim require you to preprocess and tokenize/lemmatize/etc. prior to vectorization</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document', 'first', 'four', 'is', 'longer', 'made', 'number', 'of', 'second', 'text', 'the', 'this', 'three']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer object\n",
    "vectorizer = CountVectorizer(binary=True) # See note 1 below\n",
    "corpus = [\n",
    "          'Text of first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# columns of X correspond to the result of this method\n",
    "print(vectorizer.get_feature_names()) # See Note 2 Below\n",
    "# retrieving the matrix in the numpy form\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1:** Frequency-based encoding methods like Bag of Words disregard grammar and syntax of words in the text documents. As a result, they suffer from the long tail, or Zipfian distribution, that characterizes natural language. As a result, tokens that occur very frequently are orders of magnitude more “significant” than other, less frequent ones. This can have a significant impact on some models (e.g. generalized linear models) that expect normally distributed features.\n",
    "\n",
    "The parameter (binary=True) is very important here. It allows us to use CountVectorizer as OneHotEncoding without the limitations (2D arrays vs. 1D arrays). If the parameter is left as the default (False), the matrices will show frequency of the words, not the binarized presence/absence.\n",
    "\n",
    "**Note 2:** I called .get_feature_names() to make the point that Bag of Words algorithms are efficient because when they generate the lexicon from the corpora (each term that represents a feature), <i>they are sorted alphabetically</i>. This is tremendously more computationally efficient. If you did not sort the features alphabetically, there would be no clean frame of reference and each matrix would have to be manually sorted by the greatest common number of subsequently shared documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming a new document according to the 'learned' vocabulary from corpus\n",
    "vectorizer.transform(['I have to document three of my text messages today.',\n",
    "                      'I have a document about Wendys Four for Four deals.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results of CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is representing each row as a tuple of feature names, each column is a document (word), and each cell is a word count. \n",
    "\n",
    "Each of the documents in the corpus is represented by columns of equal length. This means that in an example where a corpus might have the words \"good\" and \"great\", are just as different as \"car\" and \"tree\". This is clearly not true - and highlights an issue of the wordcount vectors generated being stripped of context.\n",
    "\n",
    "\n",
    "\n",
    "<b><u> For very large corpora:   </b></u>\n",
    "\n",
    "\n",
    "In corpora where n features and D dimensions are extremely large, it is much more efficient to use **SciKit-Learn's HashingVectorizer**. Sparse matrices have many benefits, but as they grow increasingly large, the memory utilized to store them is rapidly occupied as well. \n",
    "\n",
    "<h6 style=\"text-align: center;\"><span style=\"color: red\">Note: sklearn.preprocessing.OneHotEncoder</span></h6>\n",
    "\n",
    "The module in SciKit-Learn called OneHotEncoder is not the best fit for this specific task, despite its name. One of the goals of word embedding is dimensionality reduction to increase model performance. The OneHotEncoder treats each vector component (column) as an independent categorical variable, expanding the dimensionality of the vector for each observed value in each column. In this case, the component (four, 0) and (four, 1) would be treated as two categorical dimensions rather than as a single binary encoded vector component. \n",
    "\n",
    "OneHotEncoder is better used for categorical data encoding in pandas spreadsheets with significantly smaller n features and D dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. TF-IDF Vectorization (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words representations only describe a document in a standalone fashion, not taking into account the context (syntax/semantics) of the corpus. A different approach would be to consider the relative frequency or rareness of tokens in the document against their frequency in other documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**: TF-IDF is a very similar method to Latent Dirilecht Allocation (LDA) where weights are assigned to each word relative to their frequency in the documents. Words that appear more frequently will have a lower weight. </i></b>\n",
    "\n",
    "**Measures:** Score of each word's importance (weight) found in the corpus. \n",
    "\n",
    "**Why:** Topic Modeling and Document Classification. \"What's being talked about\" within a document or when comparing multiple corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.</b>\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.4658085493691629\n",
      "  (0, 7)\t0.4658085493691629\n",
      "  (0, 1)\t0.5908190806023349\n",
      "  (0, 0)\t0.4658085493691629\n",
      "  (1, 9)\t0.32555708674977907\n",
      "  (1, 7)\t0.32555708674977907\n",
      "  (1, 0)\t0.32555708674977907\n",
      "  (1, 10)\t0.41292788407934816\n",
      "  (1, 8)\t0.41292788407934816\n",
      "  (1, 5)\t0.41292788407934816\n",
      "  (1, 4)\t0.41292788407934816\n",
      "  (2, 6)\t0.6191302964899972\n",
      "  (2, 12)\t0.7852882757103967\n",
      "  (3, 6)\t0.41428875116588965\n",
      "  (3, 11)\t0.5254727492640658\n",
      "  (3, 3)\t0.5254727492640658\n",
      "  (3, 2)\t0.5254727492640658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "          'Text of first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "\n",
    "tfidf  = TfidfVectorizer()\n",
    "corpus = tfidf.fit_transform(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the output in the current format, the notation:\n",
    "\n",
    "**((doc, term), TFIDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the output to a sparse matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46580855 0.59081908 0.         0.         0.         0.\n",
      "  0.         0.46580855 0.         0.46580855 0.         0.\n",
      "  0.        ]\n",
      " [0.32555709 0.         0.         0.         0.41292788 0.41292788\n",
      "  0.         0.32555709 0.41292788 0.32555709 0.41292788 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.6191303  0.         0.         0.         0.         0.\n",
      "  0.78528828]\n",
      " [0.         0.         0.52547275 0.52547275 0.         0.\n",
      "  0.41428875 0.         0.         0.         0.         0.52547275\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(corpus).toarray()\n",
    "print(tfidf_matrix)\n",
    "\n",
    "## If this cell throws an AttributeError \"lower not found\", re-run the CountVectorizer cell block first. \n",
    "## Reason: tfidfVectorizer does not tokenize the corpus for you, and the above just circumnavigates it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results of TfidfVectorizer:\n",
    "\n",
    "If you do not convert the output of fit_transform() into a sparse matrix, you are provided with an list of tuples whose output corresponds with:\n",
    "\n",
    "**((doc, term), TFIDF)**\n",
    "\n",
    "Calling .toarray() transforms it into a sparse matrix where you can see the score of each term found in the documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">So how does this tie into word embedding?</h1>\n",
    "\n",
    "Methods like the ones shown above are the fundamental components that allow word embedding to work. Vectorization allows you to still build classification models such as a spam classifier using an SVM model, or for topic modeling using a Non-negative Factorization Matrix (NFM) or LDA clustering.\n",
    "\n",
    "However, the true word embedding techniques that will be explained below also bring a concept of ***distributed representations***. You'll notice there are no non-negative values generated from the methods above. This prevents us from comparing documents within a corpus that <i>don't</i> share terms. By creating a continuous vector space among all documents, we understand their similarity/disimilarity to one another. <b><u>Word embedding maintains syntactical elements and semantics.</b></u> which can also then be fed directly into deep learning neural networks for rapid, large-scale application.\n",
    "\n",
    "There are multiple methods for word embedding. Understanding the differences in functionality, scalability, and performance of each method will allow you to intuitevly decide which one is more appropriate given the problem you are working to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Types of Common Word Embedding Methods:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word2Vec\n",
    "\n",
    "**Measures:** [Cosine similarity][Ref2] between the two specified words/documents using word vectors (embeddings) of each. \n",
    "    - e.g. \"puppy\" and \"dog\" are similar, with words that surround it like \"cute\" and \"fluffy\". They would be expected to share a similar vector representation. \n",
    "    - Model is PREDICTIVE in the sense it predicts what words surround the target word.\n",
    "\n",
    "**Example Applications:** Creating a sentiment lexicon for sentiment analysis for a specific use-case like hotel review or movie reviews.\n",
    "\n",
    "**Drawbacks:** In corpora with large repititions of text documents/terms, similarity between documents are disproptionately effected for not accounting for the weighted frequencies of the terms in each document.\n",
    "\n",
    "[Word2Vec][Ref4], created by a team of researchers at Google led by Tomáš Mikolov, implements a word embedding model that enables us to create these kinds of distributed representations. It is a 2-layer shallow neural network that turns raw text into a numerical text that deep learning networks can understand and then use to predict. It is a ***feed-forward*** neural-network that can be optimized in regression/classification models using **SGD (Stochastic Gradient Descent)**.\n",
    "\n",
    "There are two methods in Word2Vec: SkipGram and ContinuousBagofWords\n",
    "\n",
    "Gensim allows us to use Word2Vec and Doc2Vec in Python, without having to adapt very large machine learning frameworks such as Keras or TensorFlow.\n",
    "\n",
    "Further explanation can be found [here][Ref3]\n",
    "\n",
    "[Ref2]: https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "[Ref3]: https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "[Ref4]: https://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [algorithm][Ref5] operates in a similar manner as the the traditional Bag of Words algorithm, but creates a distributed representation of the text documents that predict inter-similarity of documents and the likelihood a word will appear adjacent to a specified one. This gives us the ability to maintain context by predicting what words may lie adjacent to one another given a certain input.\n",
    "\n",
    "**This algorithm is feed-forward and very fast to train and execute.**\n",
    "\n",
    "[Ref5]: https://cs.stanford.edu/~quocle/paragraph_vector.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. SkipGram\n",
    "\n",
    "I highly suggest reading this [resource by Chris McCormick][Ref6] that delves deep into how the SkipGram algorithm works.\n",
    "\n",
    "The primary point I would like to make in this notebook is that, generally, SkipGram is much slower than CBOW, but considered more accurate with infrequent words. In the case of smaller datasets, the performance differential may not be as drastic. \n",
    "\n",
    "[Ref6]: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Doc2Vec\n",
    "\n",
    "The [Doc2Vec][Ref4] algorithm is an extension of Word2Vec.\n",
    "\n",
    "It proposes a paragraph vector; An unsupervised algorithm that learns fixed-length feature representations from variable length documents. \n",
    "\n",
    "Just like in Word2Vec, this distributed representation attempts to inherit the semantic properties of words such that “red” and “colorful” are more similar to each other than they are to “river” or “governance.” \n",
    "\n",
    "The additional paragraph vector takes into consideration the ordering of words within a narrow context, similar to an n-gram model. The combined result is much more effective than a bag-of-words or bag-of-n-grams model because it:\n",
    " - Generalizes better\n",
    " - Has a lower dimensionality\n",
    " - Consumes less memory than Word2Vec, as a a result of not needing to store word vectors\n",
    "\n",
    "[Ref4]: https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas Word2Vec/Doc2Vec are ***predictive*** models, [GloVe][Ref8] is **count-based.**\n",
    "\n",
    "The algorithm itself is much like [LDA (Latent Dirilecht Allocation)][Ref7] - where it generates statistical matrices of word co-occurances in a low dimensional space. However, LDA does not maintain the syntactical linear relationships in the vector space. \n",
    "\n",
    "**In a very loose sense of \"complexity\", think of TF-IDF -> LDA -> GloVe**\n",
    "\n",
    "GloVe forces the model to learn/retain the linear relationship of the text documents based on the co-occurance matrix. It accomplishes this by using a weighted least squares regression model to account for rare terms/low-term frequencies in the documents\n",
    "\n",
    "[Ref7]: https://ai.stanford.edu/~ang/papers/jair03-lda.pdf\n",
    "[Ref8]: https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Summary of Word Embedding Methods:</h1>\n",
    "\n",
    "- Word2Vec, Doc2Vec, and GloVe all have advantages/disadvantages depending on their applications \n",
    "- Common applications of word embeddings are sentiment analysis, text classification, and topic modeling\n",
    "- Word2Vec captures co-occurence of document terms one window at a time\n",
    "- Doc2Vec functions similarly, but the additinal paragraph vector allows for more targeted predictive capabilities\n",
    "- GloVe is much like LDA, but has built-in functionality to maintain syntax of the documents where LDA cannot do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
