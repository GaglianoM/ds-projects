{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><span style=\"color: #333399;\">Word2Vec and Doc2Vec for Text Classification</span></h1>\n",
    "\n",
    "<h6 style=\"text-align: center;\">Created by: Michael Gagliano on 2/8/19</h6>\n",
    "<h6 style=\"text-align: center;\">Last Update: Michael Gagliano 3/11/19</h6>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.  Overview - Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will demonstrate how to utilize Word2Vec and Doc2Vec in order to generate and apply word embeddings used for text classification.\n",
    "\n",
    "This notebook is *reference-based*, so the methods being implemented will be discussed in an appendix.\n",
    "\n",
    "References will be provided often, as to credit original processes, code, and ideas when due. As always, there can be an infinite amount of ways to solve a problem. We just want to do it:  \n",
    "<br>\n",
    "<b>\n",
    "1) Efficiently  (No need to fry your i5 or i7 processors running GridSearch locally with 1.5GB of data)  \n",
    "    \n",
    "\n",
    "\n",
    "2) Accurately  (Did we get the program to do what we wanted it to do i.e. is the estimated hypothesis as close to the target function as possible?)\n",
    "\n",
    "\n",
    "3) Elegantly   (concise code; scalable, reproducible, annoted and simple)\n",
    "</b>\n",
    "</br>\n",
    "\n",
    "***This notebook is a reference adaptation of the wonderful work done by Susan Li found [here][li]***\n",
    "\n",
    "[li]: https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Package Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction Packages from Web Pages\n",
    "import requests\n",
    "import urllib\n",
    "import bs4 as bs  \n",
    "\n",
    "# Standard Data Analytics Packages\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# NLP Packages \n",
    "import gensim\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data Visualization Packages\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\") # Sets grid to generated graphs\n",
    "sns.set_context(\"poster\") # Makes images large\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams # special matplotlib argument for improved plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Brief Review:</b></u>\n",
    "\n",
    "Text classification is ***Supervised Machine Learning***. \n",
    "\n",
    "For supervised learning, the data must contain observations (X) and labels (y).\n",
    " - The classification may be binary, or multiclass/multilabel. Methods of analysis differ between them.\n",
    " \n",
    " \n",
    " - If you need to perform classification but the data is unlabeled, you may need to perform clustering of some kind to determine underlying structure based on features (i.e. If you don't have labels, get them in a reasonable manner)\n",
    "\n",
    "A classification problem uses inductive bias to generate and narrow down generalized hypotheses (y_pred) as close as possible to the target function (y_test).\n",
    "\n",
    "In doing so, new and unseen instances can be classified by the trained classifier algorithm with (hopefully) the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching Data; Cached memory, not locally stored\n",
    "url = 'https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv'\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "\n",
    "# Saving file locally now\n",
    "with open('so_data.csv', 'wb') as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is causing this behavior  in our c# datet...</td>\n",
       "      <td>c#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have dynamic html load as if it was in an ifra...</td>\n",
       "      <td>asp.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to convert a float value in to min:sec  i ...</td>\n",
       "      <td>objective-c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.net framework 4 redistributable  just wonderi...</td>\n",
       "      <td>.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trying to calculate and print the mean and its...</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post         tags\n",
       "0  what is causing this behavior  in our c# datet...           c#\n",
       "1  have dynamic html load as if it was in an ifra...      asp.net\n",
       "2  how to convert a float value in to min:sec  i ...  objective-c\n",
       "3  .net framework 4 redistributable  just wonderi...         .net\n",
       "4  trying to calculate and print the mean and its...       python"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into pandas DataFrame object and preview first 5 rows\n",
    "df = pd.read_csv('so_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the structure of the data i.e. How many rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10286120\n"
     ]
    }
   ],
   "source": [
    "# Get a sum value of all documents (words) found in the text via the 'Post' column\n",
    "print(df['post'].apply(lambda x: len(x.split(' '))).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10,286,120 words in this document with 40,000 unique posts collected from the Stack Overflow forums. The number of features and dimensions can explode rapidly in NLP analysis, and the data we are using here is considered a very **small** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Examining Class Distribution\n",
    "\n",
    "Done to determine whether or not classes are balanced/evenly distributed.  \n",
    "If they were not, further statistical pre-processing would be needed to create valid assumptions for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "javascript       2000\n",
       "php              2000\n",
       "mysql            2000\n",
       "c#               2000\n",
       "java             2000\n",
       "android          2000\n",
       "angularjs        2000\n",
       "jquery           2000\n",
       "iphone           2000\n",
       "ruby-on-rails    2000\n",
       "python           2000\n",
       "c++              2000\n",
       "ios              2000\n",
       "asp.net          2000\n",
       "html             2000\n",
       "sql              2000\n",
       "objective-c      2000\n",
       ".net             2000\n",
       "c                2000\n",
       "css              2000\n",
       "Name: tags, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tags'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Examining Post Content\n",
    "\n",
    "Very, very rarely will mined text data be clean and ready to process right out of the gate. It's important to examine the data to get an idea of how extensive the pre-processing might be, and determining the most effective ways to accomplish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is causing this behavior  in our c# datetime type  <pre><code>[test] public void sadness() {    var datetime = datetime.utcnow;    assert.that(datetime  is.equalto(datetime.parse(datetime.tostring()))); } </code></pre>   failed :   <pre><code> expected: 2011-10-31 06:12:44.000  but was:  2011-10-31 06:12:44.350 </code></pre>   i wish to know what is happening behind the scenes in tostring() etc to cause this behavior.    edit after seeing jon s answer :   <pre><code>[test] public void newsadness() {     var datetime = datetime.utcnow;     assert.that(datetime  is.equalto(datetime.parse(datetime.tostring( o )))); } </code></pre>   result :   <pre><code>expected: 2011-10-31 12:03:04.161 but was:  2011-10-31 06:33:04.161 </code></pre>   same result with capital and small  o  . i m reading up the docs  but still unclear.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post Content for first entry in data (index pos = 0)\n",
    "df['post'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c#'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tag assignment for first entry in data (index pos = 0)\n",
    "df['tags'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is causing this behavior  in our c# datetime type  <pre><code>[test] public void sadness() {    var datetime = datetime.utcnow;    assert.that(datetime  is.equalto(datetime.parse(datetime.tostring()))); } </code></pre>   failed :   <pre><code> expected: 2011-10-31 06:12:44.000  but was:  2011-10-31 06:12:44.350 </code></pre>   i wish to know what is happening behind the scenes in tostring() etc to cause this behavior.    edit after seeing jon s answer :   <pre><code>[test] public void newsadness() {     var datetime = datetime.utcnow;     assert.that(datetime  is.equalto(datetime.parse(datetime.tostring( o )))); } </code></pre>   result :   <pre><code>expected: 2011-10-31 12:03:04.161 but was:  2011-10-31 06:33:04.161 </code></pre>   same result with capital and small  o  . i m reading up the docs  but still unclear.\n",
      "Tag: c#\n"
     ]
    }
   ],
   "source": [
    "# Make it a function \n",
    "\n",
    "def print_plot(index):\n",
    "    example = df[df.index == index][['post', 'tags']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Tag:', example[1])\n",
    "print_plot(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice here that in order to create a high-quality word embedding, we will need to remove all of the symbols, spaces, and numbers here that are not relevant to analysis. \n",
    "\n",
    "**Note:** Many of our tags contain a mix of alphanumeric characters and symbols (Example: '.net', 'c#', etc.) so we must take additional care to ensure those terms are maintained and not eliminated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Pre-Processing\n",
    "\n",
    "Regular Expression, despite many people's beliefs, is your friend here. In conjunction with the BeautifulSoup package, this makes an extremely efficient way to clean text.\n",
    "\n",
    "***IMPORTANT:*** Word2Vec, Doc2Vec, GloVe, and other word-embedding methods fall under the Text Pre-Processing section of text data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create Regular Expression Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining regular expression patterns into pattern objects, we accomplish two things:\n",
    "    1. Not having to re-write entire pattern lists\n",
    "    2. Pattern matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a compiled rule selecting:      \"[\", \"]\",\"(\", \")\", \"{\", \"}\", \"[]\", \"@\", \",\", and \";\"\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "\n",
    "# Create a rule that removes anything that IS NOT lowercase alphanumeric, and \"#\", \"+\", \"_\"\n",
    "    # In other words: Remove the remaining symbols such as:   \":\", \".\", \"-\", \"=\"\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "# Create general stopwords lexicon using pre-compile English stopwords dictionary\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Extraction with Beautiful Soup and Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding to remove HTML source elements like <post>, <code>, <pre>\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pull examples to observe results; Make adjustments if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causing behavior c# datetime type test public void sadness var datetime datetimeutcnow assertthat datetime isequalto datetimeparse datetimetostring failed expected 20111031 061244000 20111031 061244350 wish know happening behind scenes tostring etc cause behavior edit seeing jon answer test public void newsadness var datetime datetimeutcnow assertthat datetime isequalto datetimeparse datetimetostring result expected 20111031 120304161 20111031 063304161 result capital small reading docs still unclear\n",
      "Tag: c#\n"
     ]
    }
   ],
   "source": [
    "# First Row; index pos = 0\n",
    "\n",
    "df['post'] = df['post'].apply(clean_text)\n",
    "print_plot(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need interface c# possible duplicate would want use interfaces need interface want know use example interface idemo function prototype public void show first class using interface class myclass1 idemo public void show function body comes responsewrite myclass second class using interface class myclass2 idemo public void show function body comes responsewrite myclass2 responsewrite two classes function name different body even achieved without interface need interface use\n",
      "Tag: c#\n"
     ]
    }
   ],
   "source": [
    "# 9th Row; Index Pos = 10\n",
    "\n",
    "df['post'] = df['post'].apply(clean_text)\n",
    "print_plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **MUCH** better outcome. Text cleaning is extremely hard, and it will always be difficult to completely isolate words and phrases no matter how specific we make our regular expression search parameters. What we just did is an example of [Dimensionality Reduction](#Dimensionality-Reduction:); which is a way we reduce the number of variables/features in our data that are unnecessary. We can observe this via the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169018"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check updated word count found in posts\n",
    "df['post'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing our total number of features from just over 10mil prior to pre-processing down to just over 3mil is significant reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Splitting the Data for Model Development\n",
    "\n",
    "We need to now split the data into separate components entirely, so that a classification model can be built\n",
    "\n",
    "<b><u>For this notebook:</b></u>\n",
    "\n",
    "* I am splitting the Train/Test data on an 75/25 split for model building and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.post\n",
    "y = df.tags\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 30000\n",
      "testing data points: 10000\n"
     ]
    }
   ],
   "source": [
    "# Checking sizes of each data split\n",
    "print(\"training data points: {}\".format(len(y_train)))\n",
    "print(\"testing data points: {}\".format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Feature Engineering -    ** IMPORTANT **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><span style=\"color: green\">Word2Vec</h1></span>\n",
    "\n",
    "Before we actually create any text classification algorithm pipelines, we need a [word embedding](#Word-Embedding:). Word2Vec simply helps us convert large amounts of data into a word embedding for further analysis. *Applying* the Word2Vec output for methods such as Text Classification/Sentiment Analysis is what our actual goal here is.\n",
    "\n",
    "<b><u>For this notebook:</b></u>\n",
    "\n",
    "* We are using a pre-trained model from Google, containing a ***100 billion word, 3000 feature corpus*** from Google News.\n",
    "\n",
    "  \n",
    "* We are comparing technical programming forum text documents relative to this pre-trained word2vec for Google news. Do you believe this will affect the accuracy of our classification models? If so, how?\n",
    "\n",
    "<i>The pre-trained binary model can be downloaded [here][word2vecgoogle]. It is a large 1.5Gb file, so it may take awhile! </i>\n",
    "    \n",
    "Make sure it is downloaded/stored in the same working environment directory, otherwise you will need to specify a specific the specific filepath when importing the model later.\n",
    "\n",
    "[word2vecgoogle]: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n",
    "\n",
    "<b><u>Outside of this notebook:</b></u>\n",
    "\n",
    "* You can build your own pre-trained Word2Vec model, such that the model is best aligned with the focus of your text documents.\n",
    "\n",
    "See: [Appendix A](#Appendix-A:-External-References) No. 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Load Word2Vec pre-trained word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will take awhile to load ~1-3 min\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True) #Pre-computes L2 Norm for vector; correlate matrix that lowers memory load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the word2vec model holds two vectors for each word in the vocabulary: \n",
    "\n",
    "<b>\n",
    "\n",
    "1. Word embedding (rows of input/hidden matrix)  \n",
    " \n",
    "2. Context embedding (columns of hidden/output matrix)  \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common method to the Bag Of Words approach is averaging (as opposed to summing the vectors without averaging, or concatenating the vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Tokenization Check\n",
    "\n",
    "Make sure the text you are wanting to map against the Word2Vec model is tokenized. The text documents must be in tokenized form to be properly mapped to the model. If they are not, like in our current case, we will need to do it prior to applying the word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cannot compute similarity with no input ['asplabel', 'itemtemplate', 'asptextbox', 'itemtemplate', 'asptemplatefield']\n",
      "WARNING:root:cannot compute similarity with no input []\n"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql',\n",
    "           'php','ios','javascript','python','c','css','android','iphone',\n",
    "           'sql','objective-c','c++','angularjs','.net']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Deployment and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression (Multi-class text classification) - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6363333333333333\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         java       0.62      0.59      0.61       613\n",
      "         html       0.73      0.75      0.74       620\n",
      "      asp.net       0.65      0.66      0.66       587\n",
      "           c#       0.50      0.47      0.48       586\n",
      "ruby-on-rails       0.70      0.77      0.73       599\n",
      "       jquery       0.44      0.39      0.41       589\n",
      "        mysql       0.66      0.61      0.63       594\n",
      "          php       0.72      0.81      0.76       610\n",
      "          ios       0.60      0.59      0.60       617\n",
      "   javascript       0.56      0.53      0.54       587\n",
      "       python       0.55      0.51      0.53       611\n",
      "            c       0.63      0.61      0.62       594\n",
      "          css       0.65      0.65      0.65       619\n",
      "      android       0.62      0.58      0.60       574\n",
      "       iphone       0.68      0.71      0.70       584\n",
      "          sql       0.42      0.43      0.43       578\n",
      "  objective-c       0.68      0.72      0.70       591\n",
      "          c++       0.75      0.78      0.76       608\n",
      "    angularjs       0.82      0.82      0.82       638\n",
      "         .net       0.65      0.71      0.68       601\n",
      "\n",
      "    micro avg       0.64      0.64      0.64     12000\n",
      "    macro avg       0.63      0.63      0.63     12000\n",
      " weighted avg       0.63      0.64      0.63     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
    "\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
    "print(classification_report(test.tags, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Using the pre-trained Word2Vec word embedding, the accuracy of the multi-class logistic regression model is only 63.6%</b>\n",
    "\n",
    "This is certainly not ideal. Consider why this might be the case, given all that we've done so far.\n",
    "\n",
    "*Hint: Pre-processing/text cleaning, the pre-trained model context, hyperparameters in the ML model, etc.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compare to Word2Vec Embedding vs. Traditional Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.post\n",
    "y = df.tags\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Naive Bayes - Traditional CBOW method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.7425\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         java       0.65      0.64      0.65       518\n",
      "         html       0.93      0.87      0.90       507\n",
      "      asp.net       0.86      0.90      0.88       493\n",
      "           c#       0.68      0.76      0.72       479\n",
      "ruby-on-rails       0.72      0.86      0.78       495\n",
      "       jquery       0.73      0.53      0.62       484\n",
      "        mysql       0.77      0.75      0.76       490\n",
      "          php       0.69      0.91      0.78       504\n",
      "          ios       0.63      0.61      0.62       524\n",
      "   javascript       0.58      0.64      0.61       487\n",
      "       python       0.72      0.51      0.59       510\n",
      "            c       0.81      0.78      0.80       500\n",
      "          css       0.84      0.60      0.70       512\n",
      "      android       0.66      0.85      0.74       472\n",
      "       iphone       0.67      0.81      0.74       493\n",
      "          sql       0.68      0.63      0.66       496\n",
      "  objective-c       0.81      0.78      0.80       495\n",
      "          c++       0.90      0.82      0.85       511\n",
      "    angularjs       0.92      0.90      0.91       530\n",
      "         .net       0.73      0.70      0.71       500\n",
      "\n",
      "    micro avg       0.74      0.74      0.74     10000\n",
      "    macro avg       0.75      0.74      0.74     10000\n",
      " weighted avg       0.75      0.74      0.74     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Create Data Transformation Pipeline\n",
    "nb = Pipeline([('vect', CountVectorizer()), # Creates word-frequency matrix\n",
    "               ('tfidf', TfidfTransformer()), # Convert word-freq matrix into TF-IDF scored matrix\n",
    "               ('clf', MultinomialNB()), # Naive Bayes Classification\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Results:</b> Using the Naive Bayes classifier and non-Word2Vec word embedding models, **this results in a siginficantly greater model accuracy of 74%**\n",
    "\n",
    "**Why could this be?**\n",
    "\n",
    "Remember, we combined Word2Vec word embeddings from the Google News pre-trained model and the vector created with the Stack Exchange forum posts. The actual *context* of these matrices while independent are largely different; Google News covers a broad range of everything and the Stack Exchange text documents are likely to have very technical terms and vocabulary not typically seen in news articles.\n",
    "\n",
    "Naive Bayes has a high accuracy here, but that is because we did not introduce new external data to validate the model. To do this, K-Folds cross validation would be a desirable method. However, we need to keep in mind [target leakage][link1] and the implications it can have when external validation data isn't used.\n",
    "\n",
    "[link1]: https://www.datarobot.com/wiki/target-leakage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 SVM - Traditional CBOW Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.7881\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         java       0.71      0.68      0.69       518\n",
      "         html       0.83      0.94      0.88       507\n",
      "      asp.net       0.88      0.94      0.91       493\n",
      "           c#       0.83      0.75      0.79       479\n",
      "ruby-on-rails       0.75      0.87      0.80       495\n",
      "       jquery       0.79      0.41      0.54       484\n",
      "        mysql       0.82      0.69      0.75       490\n",
      "          php       0.68      0.95      0.80       504\n",
      "          ios       0.79      0.57      0.66       524\n",
      "   javascript       0.74      0.60      0.66       487\n",
      "       python       0.73      0.64      0.68       510\n",
      "            c       0.82      0.87      0.84       500\n",
      "          css       0.76      0.78      0.77       512\n",
      "      android       0.79      0.87      0.83       472\n",
      "       iphone       0.82      0.80      0.81       493\n",
      "          sql       0.72      0.70      0.71       496\n",
      "  objective-c       0.82      0.90      0.86       495\n",
      "          c++       0.83      0.96      0.89       511\n",
      "    angularjs       0.89      0.95      0.92       530\n",
      "         .net       0.78      0.88      0.83       500\n",
      "\n",
      "    micro avg       0.79      0.79      0.79     10000\n",
      "    macro avg       0.79      0.79      0.78     10000\n",
      " weighted avg       0.79      0.79      0.78     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** SVM algorithms are great for classification, and it shows its strength here. This model is the most accurate used yet at 78% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Doc2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><span style=\"color: green\">Doc2Vec</h1></span>\n",
    "\n",
    "Doc2Vec builds on the functionality of Word2Vec by allowing whole sentences/paragraphs/documents to be compared against words. The goal of this method is to provide a more accurate, clear representation of the context of each document as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Document Tagging for Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    \n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
    "\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview the tagged documents (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['fulltext', 'search', 'php', 'pdo', 'returning', 'result', 'searched', 'lot', 'matter', 'find', 'wrong', 'setup', 'trying', 'fulltext', 'search', 'using', 'pdo', 'php', 'get', 'results', 'error', 'messages', 'table', 'contains', 'customer', 'details', 'id', 'int', '11', 'auto_increment', 'name', 'varchar', '150', 'lastname', 'varchar', '150', 'company', 'varchar', '250', 'adress', 'varchar', '150', 'postcode', 'int', '5', 'city', 'varchar', '150', 'email', 'varchar', '250', 'phone', 'varchar', '20', 'orgnr', 'varchar', '15', 'timestamp', 'timestamp', 'current_timestamp', 'run', 'sqlquery', 'alter', 'table', 'system_customer', 'add', 'fulltext', 'name', 'lastname', 'except', 'columns', 'id', 'postcode', 'timestamp', 'signs', 'trouble', 'far', 'idea', 'problem', 'lies', 'db', 'configuration', 'php', 'code', 'goes', 'php', 'sth', 'dbhprepare', 'select', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'db_pre', 'customer', 'match', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'search', 'boolean', 'mode', 'bind', 'placeholders', 'sthbindparam', 'search', 'data', 'sthexecute', 'rows', 'sthfetchall', 'testing', 'print_r', 'dbherrorinfo', 'empty', 'rows', 'echo', 'else', 'echo', 'foreach', 'rows', 'row', 'echo', 'echo', 'row', 'name', 'td', 'echo', 'row', 'lastname', 'td', 'echo', 'row', 'company', 'td', 'echo', 'row', 'phone', 'td', 'echo', 'row', 'email', 'td', 'echo', 'date', 'ymd', 'strtotime', 'row', 'timestamp', 'td', 'echo', 'tr', 'echo', 'tried', 'change', 'parameter', 'searchquery', 'string', 'like', 'testcompany', 'somename', 'boolean', 'mode', 'also', 'read', 'word', 'found', '50', 'rows', 'counts', 'common', 'word', 'pretty', 'sure', 'case', 'uses', 'specific', 'words', 'table', 'uses', 'myisam', 'engine', 'get', 'results', 'error', 'messages', 'please', 'help', 'point', 'wrong', 'thank'], tags=['Train_0']),\n",
       " TaggedDocument(words=['select', 'everything', '1', 'table', 'x', 'rows', 'another', 'im', 'making', 'join', 'query', 'like', 'select', 'clothes', 'c', 'join', 'style', 'cstyleid', 'ssylelid', 'clothesid', '19', 'dont', 'want', 'select', 'everything', 'style', 'want', 'select', 'everything', 'clothes', '20', 'rows', 'select', '1', 'row', '10', 'style', 'easyest', 'way', 'without', 'select', 'every', 'row', 'clothes', '20', 'things', 'select', 'like', 'select', 'cid', 'cdescription', 'cname', 'csize', 'cbrand', 'sname', 'clothes', 'c', 'join', 'style', 'cstyleid', 'stsylelid', 'clothesid', '19', 'would', 'fastest', 'way', 'possibillity'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the doc2vec, we will vary the following parameters:\n",
    "\n",
    "> dm=0 , distributed bag of words (DBOW) is used.\n",
    "\n",
    "\n",
    "> vector_size=300 , 300 vector dimensional feature vectors.\n",
    "\n",
    "\n",
    "> negative=5 , specifies how many “noise words” should be drawn.\n",
    "\n",
    "\n",
    "> min_count=1, ignores all words with total frequency lower than this.\n",
    "\n",
    "\n",
    "> alpha=0.065 , the initial learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create a Distributed Representation Model and Train over 30 Iterations of Shuffled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 4003917.71it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 4011193.04it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2809877.40it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1503294.36it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2570984.43it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2781368.70it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2675535.99it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2864961.75it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 4264020.74it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1604308.45it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2076156.86it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2275246.96it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2673787.75it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2856960.70it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2708099.17it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1526408.71it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2669745.71it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2285817.68it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1655929.57it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2348270.14it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1910409.47it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1456431.41it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1418096.49it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2295073.39it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2620293.62it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1546828.94it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2037058.77it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2033355.47it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2029272.82it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 2282831.83it/s]\n",
      "100%|██████████████████████████████████████████| 40000/40000 [00:00<00:00, 1907607.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Shows progress bar when training over documents\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from sklearn import utils\n",
    "\n",
    "# Build Model\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "# Train model over 30 iterations of shuffled data\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Extract the trained Doc2Vec Word Embeddings for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Logistic Regression Model Analysis - Doc2Vec Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mike Gagliano\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8036666666666666\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         java       0.69      0.68      0.69       589\n",
      "         html       0.89      0.91      0.90       661\n",
      "      asp.net       0.93      0.94      0.93       606\n",
      "           c#       0.77      0.77      0.77       613\n",
      "ruby-on-rails       0.84      0.89      0.86       601\n",
      "       jquery       0.71      0.71      0.71       585\n",
      "        mysql       0.87      0.81      0.84       621\n",
      "          php       0.81      0.83      0.82       587\n",
      "          ios       0.67      0.66      0.67       560\n",
      "   javascript       0.69      0.65      0.67       611\n",
      "       python       0.66      0.67      0.67       593\n",
      "            c       0.78      0.83      0.81       581\n",
      "          css       0.82      0.75      0.78       608\n",
      "      android       0.84      0.84      0.84       593\n",
      "       iphone       0.84      0.82      0.83       592\n",
      "          sql       0.72      0.66      0.69       597\n",
      "  objective-c       0.85      0.89      0.87       604\n",
      "          c++       0.89      0.93      0.91       610\n",
      "    angularjs       0.94      0.96      0.95       595\n",
      "         .net       0.80      0.83      0.82       593\n",
      "\n",
      "    micro avg       0.80      0.80      0.80     12000\n",
      "    macro avg       0.80      0.80      0.80     12000\n",
      " weighted avg       0.80      0.80      0.80     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: External References\n",
    "\n",
    "[1. RegEx Tester][Ref1]\n",
    "\n",
    "[Ref1]: https://regexr.com/\n",
    "\n",
    "[2. Building Your Own Custom Word Embedding for a Word2Vec Model][Ref2]\n",
    "\n",
    "[Ref2]: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "[3. Word2Vec for Recommender Tags and Advertising][Ref3]\n",
    "\n",
    "[Ref3]: http://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/\n",
    "\n",
    "[4.a Word2Vec's init_sims() method][Ref4]\n",
    "\n",
    "[Ref4]: https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims\n",
    "\n",
    "[5. Doc2Vec Sentence and Paragraph Tagging for Pre-Processing via TaggedDocument method][Ref5]\n",
    "\n",
    "[Ref5]: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "[6. FastText API, an NLP Framework for Text Classification][Ref6]\n",
    "\n",
    "[Ref6]: https://www.analyticsvidhya.com/blog/2017/07/word-representations-text-classification-using-fasttext-nlp-facebook/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C: Terms and External References\n",
    "\n",
    "[Return to Start of Notebook](#Text-Classification)\n",
    "\n",
    "#### Word Embedding: \n",
    "\n",
    "A representation of text where words that have the same meaning have a similar representation. In other words it represents words in a coordinate system where related words, based on a corpus of relationships, are placed closer together.\n",
    "\n",
    "#### Dimensionality Reduction:\n",
    "\n",
    "See: [link][ref1]\n",
    "\n",
    "[ref1]: https://en.wikipedia.org/wiki/Dimensionality_reduction\n",
    "\n",
    "\n",
    "#### Target Leakage:\n",
    "\n",
    "Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed. \n",
    "\n",
    "See: [link][link2]\n",
    "\n",
    "[link2]: https://machinelearningmastery.com/data-leakage-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
